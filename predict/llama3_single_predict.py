import torch
from transformers import AutoModelForCausalLM, AutoTokenizer


def get_model_result(base_model_path):
    tokenizer = AutoTokenizer.from_pretrained(base_model_path)
    device = "cuda"

    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        device_map="auto",
        torch_dtype=torch.bfloat16,
    ).eval()

    prompt = "è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚ä½ æ˜¯è°ï¼Ÿ"


    """
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a pirate chatbot who always responds in pirate speak!<|eot_id|><|start_header_id|>user<|end_header_id|>

è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚ä½ æ˜¯è°ï¼Ÿ<|eot_id|><|start_header_id|>assistant<|end_header_id|>

ğŸ˜Šæˆ‘æ˜¯ LLaMAï¼Œä¸€ä¸ªç”± Meta å¼€å‘çš„äººå·¥æ™ºèƒ½è¯­è¨€æ¨¡å‹ã€‚æˆ‘å¯ä»¥ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ï¼Œå¸®åŠ©å›ç­”é—®é¢˜ã€ç”Ÿæˆæ–‡æœ¬ã€è¿›è¡Œå¯¹è¯ç­‰ã€‚æˆ‘çš„èƒ½åŠ›åŒ…æ‹¬ä½†ä¸é™äºè‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­è¨€ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸã€‚æˆ‘å¾ˆé«˜å…´å’Œä½ äº¤æµï¼Œå›ç­”ä½ çš„é—®é¢˜å’Œè®¨è®ºæœ‰è¶£çš„è¯é¢˜ï¼ ğŸ˜Š<|eot_id|><|start_header_id|>user<|end_header_id|>

è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚ç½‘å§å¯ä»¥ä¸Šç½‘ï¼Œå¼±æ™ºå§ä¸ºä»€ä¹ˆä¸å¯ä»¥ä¸Šå¼±æ™ºï¼Ÿ<|eot_id|><|start_header_id|>assistant<|end_header_id|>
    """

    messages = [
        {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
        {"role": "user", "content": prompt},
        {"role": "assistant", "content":"ğŸ˜Šæˆ‘æ˜¯ LLaMAï¼Œä¸€ä¸ªç”± Meta å¼€å‘çš„äººå·¥æ™ºèƒ½è¯­è¨€æ¨¡å‹ã€‚æˆ‘å¯ä»¥ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ï¼Œå¸®åŠ©å›ç­”é—®é¢˜ã€ç”Ÿæˆæ–‡æœ¬ã€è¿›è¡Œå¯¹è¯ç­‰ã€‚æˆ‘çš„èƒ½åŠ›åŒ…æ‹¬ä½†ä¸é™äºè‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­è¨€ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸã€‚æˆ‘å¾ˆé«˜å…´å’Œä½ äº¤æµï¼Œå›ç­”ä½ çš„é—®é¢˜å’Œè®¨è®ºæœ‰è¶£çš„è¯é¢˜ï¼ ğŸ˜Š"},
        {"role": "user", "content": "è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚ç½‘å§å¯ä»¥ä¸Šç½‘ï¼Œå¼±æ™ºå§ä¸ºä»€ä¹ˆä¸å¯ä»¥ä¸Šå¼±æ™ºï¼Ÿ"}
    ]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    print(text)

    model_inputs = tokenizer([text], return_tensors="pt").to(device)

    def get_result(model_inputs, model):
        generated_ids = model.generate(
            model_inputs.input_ids,
            max_new_tokens=512,
            eos_token_id=tokenizer.get_vocab()["<|eot_id|>"]
        )
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]

        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0]
        return response

    base_model_response = get_result(model_inputs, base_model)
    print("ç»“æœï¼š", base_model_response)


if __name__ == '__main__':
    base_path = "../model_hub/LLM-Research/Meta-Llama-3-8B-Instruct/"
    get_model_result(base_path)
